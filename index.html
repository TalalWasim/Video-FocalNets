<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition.">
  <meta name="keywords" content="Videos, Spatio-temporal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!---->


 <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://muzairkhattak.github.io/">Syed Talal Wasim</a><sup>*, 1</sup>,</span>
            <span class="author-block">
              <a href="https://talalwasim.github.io/">Muhammad Uzair Khattak</a><sup>*, 1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=tM9xKA8AAAAJ&hl=en&oi=ao">Muzammal Naseer,</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://salman-h-khan.github.io/">Salman Khan</a><sup>1, 2</sup>,
            </span>
            <span class="author-block">
              <a href="http://faculty.ucmerced.edu/mhyang/"> Mubarak Shah</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.es/citations?user=zvaeYnUAAAAJ&hl=en">Fahad Shahbaz Khan</a><sup>1, 3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Mohamed bin Zayed University of AI,</span>
            <span class="author-block"><sup>2</sup>Australian National University,</span>
            <span class="author-block"><sup>3</sup>Linkoping University,</span>
            <span class="author-block"><sup>4</sup>University of Central Florida,</span>
          </div>
                    <div class="is-size-5 publication-authors">
               <span class="author-block"> <b> *Joint first authors</b>  </span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/TalalWasim/Video-FocalNets"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
<p align="center">
  <img alt="Overall Architecture" src="static/images/intro_plot.png" width="1200"/>
  <p align="justify"><b>Accuracy vs Computational Complexity trade-off comparison:</b> We show the performance of Video-FocalNets against recent Methods for video action recognition. Accuracy is compared on the Kinetics-400 dataset against GFLOPs/view. Our Video-FocalNets perform favorably compared to their counterparts across a range of model sizes (Tiny, Small, and Base).</p>
</p>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice. Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on three large-scale datasets (Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost.  Our code/models are publicly released.           <br>

          </p>

                  <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
           <h2 class="title is-3 has-text-centered">Visualizations</h2>
          <div class="content has-text-justified">

                 <p align=""justify="">
            We visualize the spatial and temporal modulators for sample videos from Kinetics-600 and Something-Something-V2. Note how the temporal modulator fixates on the <i>global</i> motion across frames while the spatial modulator captures <i>local</i> variations.
          </p>
              </div>


        </div>
      </div>
      <!--/ Visual Effects. -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>

</section>



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
<img alt="Performance Comparison" src="static/images/vis_images/cutting_apple.png" width="98%">
        </div>
        <div class="item item-chair-tp">
<img alt="Performance Comparison" src="static/images/vis_images/Pushing_something_so_that_it_slightly_moves.png" width="98%">
        </div>
        <div class="item item-shiba">
<img alt="Performance Comparison" src="static/images/vis_images/Putting_something_on_a_flat_surface_without_letting_it_roll.png" width="98%">
        </div>
        <div class="item item-fullbody">
<img alt="Performance Comparison" src="static/images/vis_images/Showing_that_something_is_empty.png" width="98%">
        </div>
        <div class="item item-blueshirt">
<img alt="Performance Comparison" src="static/images/vis_images/smashing.png" width="98%">
        </div>
        <div class="item item-mask">
<img alt="Performance Comparison" src="static/images/vis_images/tying_knot_not_on_a_tie.png" width="98%">
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Video-FocalNets architecture</h2>



        <div class="content has-text-centered">
            <img src="./static/images/overall_architecture.png">
        </div>
        <div class="content has-text-justified">
          <p>
            <b>(a) The overall architecture of Video-FocalNets:</b>  A four-stage architecture, with each stage comprising a patch embedding and a number of Video-FocalNet blocks. <b>(b) Single Video-FocalNet block:</b> Similar to the transformer blocks, we replace self-attention with Spatio-Temporal Focal Modulation.
          </p>
        </div>

      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
<!--        <h2 class="title is-3">PromptSRC results comparison</h2>-->

        <div class="content has-text-centered">
            <img src="./static/images/overview_focal_modulation.png" width="50%">
        </div>
        <div class="content has-text-justified">
          <p>
            <b>The Spatio-Temporal Focal Modulation layer:</b> A spatio-temporal focal modulation block that independently models the spatial and temporal information.
          </p>
          <h2 class="title is-3 has-text-centered">Comparisons with State-of-The-Art</h2>
        </div>
                <div class="content has-text-justified">

                  <p align="justify"> Video-FocalNets shows consist improvements across multiple large scale benchmarks. We present results below for the Kinetics-400, Kinetics-600 and Something-Something-v2 datasets.</p>

                   <h3 class="title is-4 has-text-justified">Results on Kinetics-400</h3>

                  <center>
<table  border="0">
<tbody>
<tr>
        <td><b>Method</b></td>
        <td><center><b>Pre-training</b></td>
        <td><center><b>Top-1</b></center></td>
        <td><center><b>Views</b></center></td>
        <td><center><b>FLOPs (G/view)</b></center></td>
    </tr>
    <tr>
        <td>MTV-B (CVPR'22)</td>
        <td><center>ImageNet-21K</center></td>
        <td><center>81.8</center></td>
        <td><center>4 x 3</center></td>
        <td><center>399</center></td>
    </tr>
    <tr>
        <td>MTV-B (320p) (CVPR'22)</td>
        <td><center>ImageNet-21K</center></td>
        <td><center>82.4</center></td>
        <td><center>4 x 3</center></td>
        <td><center>967</center></td>
    </tr>
    <tr>
        <td>Video-Swin-T (CVPR'22)</td>
        <td><center>ImageNet-1K</center></td>
        <td><center>78.8</center></td>
        <td><center>4 x 3</center></td>
        <td><center>88</center></td>
    </tr>
    <tr>
        <td>Video-Swin-S (CVPR'22)</td>
        <td><center>ImageNet-1K</center></td>
        <td><center>80.6</center></td>
        <td><center>4 x 3</center></td>
        <td><center>166</center></td>
    </tr>
    <tr>
        <td>Video-Swin-B (CVPR'22)</td>
        <td><center>ImageNet-1K</center></td>
        <td><center>80.6</center></td>
        <td><center>4 x 3</center></td>
        <td><center>282</center></td>
    </tr>
    <tr>
        <td>Video-Swin-B (CVPR'22)</td>
        <td><center>ImageNet-21K</center></td>
        <td><center>82.7</center></td>
        <td><center>4 x 3</center></td>
        <td><center>282</center></td>
    </tr>
    <tr>
        <td>MViTv2-B (CVPR'22)</td>
        <td><center>-</center></td>
        <td><center>82.9</center></td>
        <td><center>5 x 1</center></td>
        <td><center>226</center></td>
    </tr>
    <tr>
        <td>Uniformer-B (ICLR'22)</td>
        <td><center>ImageNet-1K</center></td>
        <td><center>83.0</center></td>
        <td><center>4 x 3</center></td>
        <td><center>259</center></td>
    </tr>
    <tr>
        <td>Video-FocalNet-T</td>
        <td><center>ImageNet-1K</center></td>
        <td><center>79.8</center></td>
        <td><center>4 x 3</center></td>
        <td><center>63</center></td>
    </tr>
    <tr>
        <td>Video-FocalNet-S</td>
        <td><center>ImageNet-1K</center></td>
        <td><center>81.4</center></td>
        <td><center>4 x 3</center></td>
        <td><center>124</center></td>
    </tr>
    <tr>
        <td>Video-FocalNet-B</td>
        <td><center>ImageNet-1K</center></td>
        <td><center><b>83.6</b></center></td>
        <td><center>4 x 3</center></td>
        <td><center>149</center></td>
    </tr>

</tbody>
</table>
</center>

                       <h3 class="title is-4 has-text-justified">Results on Kinetics-600</h3>

                    <center>
<table  border="0">
<tbody>
<tr>
    <td><b>Method</b></td>
    <td><center><b>Pre-training</b></center></td>
    <td><center><b>Top-1</b></center></td>
</tr>
<tr>
    <td>MTV-B (CVPR'22)</td>
    <td><center>ImageNet-21K</center></td>
    <td><center>83.6</center></td>
</tr>
<tr>
    <td>MTV-B (320p) (CVPR'22)</td>
    <td><center>ImageNet-21K</center></td>
    <td><center>84.0</center></td>
</tr>
<tr>
    <td>Video-Swin-B (CVPR'22)</td>
    <td><center>ImageNet-21K</center></td>
    <td><center>84.0</center></td>
</tr>
<tr>
    <td>Uniformer-B (ICLR'22)</td>
    <td><center>ImageNet-1K</center></td>
    <td><center>84.5</center></td>
</tr>
<tr>
    <td>MoViNet-A6 (CVPR'21)</td>
    <td><center>ImageNet-21K</center></td>
    <td><center>84.8</center></td>
</tr>
<tr>
    <td>MViTv2-B (CVPR'22)</td>
    <td><center>-</center></td>
    <td><center>85.5</center></td>
</tr>
<tr>
    <td>Video-FocalNet-B</td>
    <td><center>ImageNet-1K</center></td>
    <td><center><b>86.7</b></center></td>
</tr>
</tbody>
</table>
</center>


                       <h3 class="title is-4 has-text-justified">Results on Something-Something V2</h3>

                    <center>
<table  border="0">
<tbody>
<tr>
    <td><b>Method</b></td>
    <td><center><b>Pre-training</b></center></td>
    <td><center><b>Top-1</b></center></td>
</tr>
<tr>
    <td>MTV-B (CVPR'22)</td>
    <td><center>ImageNet-21K</center></td>
    <td><center>67.6</center></td>
</tr>
<tr>
    <td>MTV-B (320p) (CVPR'22)</td>
    <td><center>ImageNet-21K</center></td>
    <td><center>68.5</center></td>
</tr>
<tr>
    <td>Video-Swin-B (CVPR'22)</td>
    <td><center>Kinetics400</center></td>
    <td><center>69.6</center></td>
</tr>
<tr>
    <td>Uniformer-B (ICLR'22)</td>
    <td><center>Kinetics400</center></td>
    <td><center>70.4</center></td>
</tr>
<tr>
    <td>MViTv2-B (CVPR'22)</td>
    <td><center>Kinetics400</center></td>
    <td><center>70.5</center></td>
</tr>
<tr>
    <td>Video-FocalNet-B</td>
    <td><center>Kinetics400</center></td>
    <td><center><b>71.1</b></center></td>
</tr>

</tbody>
</table>
</center>



        <h3 class="title is-4 has-text-justified">Conclusion</h3>
        <div class="content has-text-justified">
          <p>
To learn spatio-temporal representations that can effectively model both local and global contexts, this paper introduces Video-FocalNets for video action recognition tasks. This architecture is derived from focal modulation for images and is able to effectively model both short- and long-term dependencies to learn strong spatio-temporal representations. We extensively evaluate several design choices to develop our proposed Video-FocalNet block. Specifically, our Video-FocalNet uses a parallel design to model hierarchical contextualization by combining spatial and temporal convolution and multiplication operations in a computationally efficient manner. Video-FocalNets are more efficient than transformer-based architectures which require expensive self-attention operations. We demonstrate the effectiveness of Video-FocalNets via evaluations on three representative large-scale video datasets, where our approach outperforms previous transformer- and CNN-based Methods.</p>
     <br><p>For more details about the proposed video recognition framework and results/comparisons over additional benchmarks, please refer to our main paper. Thank you!</p>
        </div>

</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wasim2023videofocalnets,
    title={Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition},
    author={Syed Talal Wasim and Muhammad Uzair Khattak and Muzammal Naseer and Salman Khan and Mubarak Shah and Fahad Shahbaz Khan},
    journal={arXiv:},
    year={2023}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
